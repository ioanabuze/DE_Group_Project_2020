{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD TO THIS FOLDER WHEN YOU ARE SURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE THIS CODE IF YOU GET LOST OR ADD STUFF THAT YOU FEEL WOULD BE USEFUL FOR EVERYONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IF IMPORTS FAIL, IMPORT THE FOLLOWING\n",
    "#! pip install pyspark\n",
    "#! pip install snorkel\n",
    "#! pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries, OS and Setup \n",
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling import LFAnalysis\n",
    "#from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "number_cores = 8\n",
    "memory_gb = 24\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setMaster('local[{}]'.format(number_cores))\n",
    "        .set('spark.driver.memory', '{}g'.format(memory_gb))\n",
    ")\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Data\n",
    "Read Dataset\n",
    "\n",
    "Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Data\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "amazon_reviews_dataset_sql_context = sqlContext.read.csv(\"/project/DE_Group_Project_2020/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\",header=True)\n",
    "amazon_reviews_dataset_pd_df = pd.read_csv(\"/project/DE_Group_Project_2020/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\")\n",
    "amazon_reviews_dataset_rdd = amazon_reviews_dataset_sql_context.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Writing Labeling Functions\n",
    "\n",
    "a) Exploring the training set for initial ideas\n",
    "\n",
    "b) Writing LFs\n",
    "\n",
    "c) Evaluate performance on training set\n",
    "\n",
    "d) Balance accuracy and coverage\n",
    "\n",
    "e) Writing an LF that uses a third-party model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, for each rating tier, we want to visualize the words that seem to appear the most. These most common words will then be used to determine whether positive reviews are contained in the last 3 rating tiers and negative reviews are contained in the first two rating tiers.\n",
    "The first rating tier (1) is considered to be consistent of really negative reviews. The following graph, helps us understand the words that seem to appear in this specific tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the 5 different reviews WordCloud and figure other ways to quantify frequency of words\n",
    "# From - https://towardsdatascience.com/spam-classifier-in-python-from-scratch-27a98ddd8e73\n",
    "\n",
    "# WorldCloud - Review == 1 \n",
    "reviewone = ' '.join(list(amazon_reviews_dataset_pd_df[amazon_reviews_dataset_pd_df['reviews.rating']==1]['reviews.text']))\n",
    "reviewone_wc = WordCloud(width=512,height=512).generate(reviewone)\n",
    "\n",
    "plt.figure(figsize = (10,8), facecolor = 'k')\n",
    "plt.imshow(reviewone_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second rating tier (2) is considered to be consistent of less negative reviews compared to the first tier. The following graph, helps us understand the words that seem to appear in this specific tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WorldCloud - Review == 2 \n",
    "reviewone = ' '.join(list(amazon_reviews_dataset_pd_df[amazon_reviews_dataset_pd_df['reviews.rating']==2]['reviews.text']))\n",
    "reviewone_wc = WordCloud(width=512,height=512).generate(reviewone)\n",
    "\n",
    "plt.figure(figsize = (10,8), facecolor = 'k')\n",
    "plt.imshow(reviewone_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third rating tier (3) is considered to be consistent of positive reviews but not that pleasing . The following graph, helps us understand the words that seem to appear in this specific tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WorldCloud - Review == 3 \n",
    "reviewone = ' '.join(list(amazon_reviews_dataset_pd_df[amazon_reviews_dataset_pd_df['reviews.rating']==3]['reviews.text']))\n",
    "reviewone_wc = WordCloud(width=512,height=512).generate(reviewone)\n",
    "\n",
    "plt.figure(figsize = (10,8), facecolor = 'k')\n",
    "plt.imshow(reviewone_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth rating tier (4) is considered to be consistent of more positive reviews compared to the third tier. The following graph, helps us understand the words that seem to appear in this specific tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'amazon_reviews_dataset_pd_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c14a2d7848ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# WorldCloud - Review == 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreviewone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamazon_reviews_dataset_pd_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mamazon_reviews_dataset_pd_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviews.rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviews.text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreviewone_wc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviewone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'amazon_reviews_dataset_pd_df' is not defined"
     ]
    }
   ],
   "source": [
    "# WorldCloud - Review == 4 \n",
    "reviewone = ' '.join(list(amazon_reviews_dataset_pd_df[amazon_reviews_dataset_pd_df['reviews.rating']==4]['reviews.text']))\n",
    "reviewone_wc = WordCloud(width=512,height=512).generate(reviewone)\n",
    "\n",
    "plt.figure(figsize = (10,8), facecolor = 'k')\n",
    "plt.imshow(reviewone_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fifth rating tier (5) is considered to be consistent of the most positive reviews. The following graph, helps us understand the words that seem to appear in this specific tier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WorldCloud - Review == 5\n",
    "reviewone = ' '.join(list(amazon_reviews_dataset_pd_df[amazon_reviews_dataset_pd_df['reviews.rating']==5]['reviews.text']))\n",
    "reviewone_wc = WordCloud(width=512,height=512).generate(reviewone)\n",
    "\n",
    "plt.figure(figsize = (10,8), facecolor = 'k')\n",
    "plt.imshow(reviewone_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define constants to represent the class labels for:\n",
    "- Rating one = 0\n",
    "- Rarting two = 1\n",
    "- Rarting three = 2\n",
    "- Rating four = 3\n",
    "- Rating five = 4\n",
    "- Abstaining: -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set voting values.\n",
    "RATINGONE = 0\n",
    "RATINGTWO = 1 \n",
    "RATINGTHREE = 2\n",
    "RATINGFOUR = 3\n",
    "RATINGFIVE = 4\n",
    "\n",
    "ABSTAIN = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now write labelling functions to determine whether positive reviews are contained in the last three rating tiers and negative reviews are contained in the first two rating tiers as expected by human intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labeling_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ff1ce98a410b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# labelling functions for five different rating tiers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mlabeling_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mRATINGTHREE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRATINGFOUR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRATINGFIVE\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mPOSITIVE\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mABSTAIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labeling_function' is not defined"
     ]
    }
   ],
   "source": [
    "# Common posititve words \n",
    "POSITIVE = r\"\\bjew (love|great|good|easy|happy|nice|fun|pretty|works|better|well|ok)\"\n",
    "# Common negative words\n",
    "NEGATIVE = r\"\\bjew (old|junk|useless|failded|slow|nothing)\"\n",
    "\n",
    "# labelling functions for five different rating tiers \n",
    "@labeling_function()\n",
    "def positive(x):\n",
    "  return RATINGTHREE, RATINGFOUR, RATINGFIVE if POSITIVE in x.text.lower() else ABSTAIN\n",
    "\n",
    "@labeling_function()\n",
    "def negative(x):\n",
    " return RATINGONE, RATINGTWO if NEGATIVE in x.text.lower() else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the created labelling functions to a collection of data points, we use an LFApplier. In this case, we use the SparkLFApplier because our data points are represented with a RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label matrix\n",
    "\n",
    "from snorkel.labeling.apply.spark import SparkLFApplier\n",
    "\n",
    "lfs = [positive, negative]\n",
    "\n",
    "applier = SparkLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(data_points = amazon_reviews_dataset_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the coverage of the created labelling functions, the percentage of the dataset that they label in order to evaluate the performance of our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_positive, coverage_negative = (L_train != ABSTAIN).mean(axis=0)\n",
    "print(f\"positive coverage: {coverage_positive * 100:.1f}%\")\n",
    "print(f\"negative coverage: {coverage_negative * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFAnalysis(L=applied, lfs=labelingfunctions).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Writing More Labeling Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Keyword LFs\n",
    "\n",
    "b) Pattern-matching LFs (regular expressions)\n",
    "\n",
    "c) Heuristic LFs\n",
    "\n",
    "d) LFs with Complex Preprocessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now move forward with out analysis and start writting labelling functions that use a third-party model.\n",
    "The TextBlob tool suppliers a pretrained sentiment analyzer. The positive and negative classification tasks are not the same as sentiment classifications. We might think that rating tiers have different distributions of sentiment scores (Snorkel.org, 2020).\n",
    "We start by developing a Preprocessor that runs TextBlob on our review comments and later, obtains the polarity and subjectivity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.preprocess import preprocessor\n",
    "from textblob import TextBlob\n",
    "\n",
    "# extract polarity and subjectivity scores \n",
    "@preprocessor(memoize=True)\n",
    "def textblob_sentiment(x):\n",
    "    scores = TextBlob(x.text)\n",
    "    x.polarity = scores.sentiment.polarity\n",
    "    x.subjectivity = scores.sentiment.subjectivity\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we focus on writing labelling functions for the last three rating tiers which are though to contain mostly positive reviews comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_polarity(x):\n",
    "    return RATINGTHREE, RATINGFOUR, RATINGFIVE if x.polarity > 0.9 else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_subjectivity(x):\n",
    "    return RATINGTHREE, RATINGFOUR, RATINGFIVE if x.subjectivity >= 0.5 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to apply our labelling functions so we can analyze their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling.apply.spark import SparkLFApplier\n",
    "lfs = [textblob_polarity, textblob_subjectivity]\n",
    "\n",
    "applier = SparkLFApplier(lfs)\n",
    "applied = applier.apply(data_points=amazon_reviews_dataset_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LFAnalysis(L=applied, lfs=labelingfunctions).lf_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we focus on writing labelling functions for the first two rating tiers which are though to contain mostly negative reviews comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_polarity(x):\n",
    "    return RATINGONE, RATINGTWO if x.polarity <= 0.9 else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function(pre=[textblob_sentiment])\n",
    "def textblob_subjectivity(x):\n",
    "    return RATINGONE, RATINGTWO if x.subjectivity < 0.5 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we to apply our labelling functions so we can analyze their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [textblob_polarity, textblob_subjectivity]\n",
    "\n",
    "applier = PandasLFApplier(lfs)\n",
    "L_train = applier.apply(amazon_reviews_dataset_pd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Combining Labeling Function Outputs with the Label Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate Performance on Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Filtering out unlabeled data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Featurization\n",
    "\n",
    "Scikit-Learn Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
